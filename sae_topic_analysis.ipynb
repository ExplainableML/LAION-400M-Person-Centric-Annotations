{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d556e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils.dictionary_learning.trainers.batch_top_k import BatchTopKSAE\n",
    "from utils.dictionary_learning.trainers.top_k import AutoEncoderTopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d449b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"ibm-granite/granite-embedding-english-r2\"\n",
    "\n",
    "# Choose device automatically\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedder = SentenceTransformer(model_name, trust_remote_code=True, device=device, model_kwargs={'torch_dtype': torch.bfloat16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3b4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def cluster_by_similarity(\n",
    "    topic_labels: List[str],\n",
    "    similarity_matrix: np.ndarray,\n",
    "    threshold: float\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Partitions topic labels into clusters based on a similarity threshold.\n",
    "\n",
    "    A cluster is formed such that the pairwise similarity of all labels\n",
    "    within that cluster is higher than the given threshold. This function\n",
    "    uses a greedy approach to form the clusters.\n",
    "\n",
    "    Args:\n",
    "        topic_labels: A list of string labels.\n",
    "        similarity_matrix: A 2D numpy array where matrix[i, j] is the\n",
    "                           cosine similarity between topic_labels[i] and\n",
    "                           topic_labels[j].\n",
    "        threshold: The similarity threshold (tau) for clustering. A pair is\n",
    "                   considered similar if their similarity is > threshold.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list is a cluster of topic labels.\n",
    "        The inner lists are sorted for consistent output.\n",
    "    \"\"\"\n",
    "    num_labels = len(topic_labels)\n",
    "    if num_labels == 0:\n",
    "        return []\n",
    "\n",
    "    # Keep track of the indices of labels that have not yet been clustered.\n",
    "    unclustered_indices = set(range(num_labels))\n",
    "    clusters = []\n",
    "\n",
    "    # Iterate through all labels to potentially start a new cluster.\n",
    "    # We use a sorted list to ensure the output is deterministic.\n",
    "    sorted_initial_indices = sorted(list(unclustered_indices))\n",
    "\n",
    "    for i in sorted_initial_indices:\n",
    "        # If the label has already been assigned to a cluster, skip it.\n",
    "        if i not in unclustered_indices:\n",
    "            continue\n",
    "\n",
    "        # Start a new cluster with the current label.\n",
    "        current_cluster_indices = [i]\n",
    "        unclustered_indices.remove(i)\n",
    "\n",
    "        # Iterate through other unclustered labels to see if they can join.\n",
    "        # We iterate over a sorted copy as the set is modified during the loop.\n",
    "        potential_members = sorted(list(unclustered_indices))\n",
    "        for j in potential_members:\n",
    "            if j not in unclustered_indices:\n",
    "                continue\n",
    "\n",
    "            # A candidate label 'j' can join if it is similar enough to ALL\n",
    "            # existing members of the current cluster.\n",
    "            is_compatible = True\n",
    "            for member_idx in current_cluster_indices:\n",
    "                if similarity_matrix[j, member_idx] <= threshold:\n",
    "                    is_compatible = False\n",
    "                    break\n",
    "            \n",
    "            if is_compatible:\n",
    "                # If compatible, add the label to the cluster and mark it as clustered.\n",
    "                current_cluster_indices.append(j)\n",
    "                unclustered_indices.remove(j)\n",
    "        \n",
    "        # Convert the indices in the completed cluster back to labels.\n",
    "        new_cluster = [topic_labels[idx] for idx in current_cluster_indices]\n",
    "        clusters.append(sorted(new_cluster))\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0047c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630271c1162b40329e73116bea291e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"lexical_data/topic_labels.txt\", \"r\") as f:\n",
    "    topic_labels = list(sorted(set([line.strip() for line in f.readlines()])))\n",
    "    topic_labels.remove(\"missing person\")\n",
    "\n",
    "topic_label_embeddings = embedder.encode(topic_labels, batch_size=4096, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "cdist_cos = torch.matmul(topic_label_embeddings, topic_label_embeddings.T).float().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae9a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "identities = os.listdir(\"results/intersectional_statistics/all_images_by_intersectional_identity/\")\n",
    "identities = list(sorted([identity.replace(\".txt\", \"\") for identity in identities]))\n",
    "\n",
    "selected_identities = []\n",
    "for identity in identities:\n",
    "    identity_parts = identity.split(\"_\")\n",
    "    gender = identity_parts[1]\n",
    "    race = identity_parts[0]\n",
    "\n",
    "    if \"nopair\" in identity:\n",
    "        continue\n",
    "    \n",
    "    if gender in [\"male\", \"female\"] and race not in [\"mixed\", \"unclear\"]:\n",
    "        selected_identities.append(identity)\n",
    "\n",
    "selected_identities = list(sorted(selected_identities))\n",
    "selected_identity_indices = [identities.index(identity) for identity in selected_identities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb45eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090d47e6d0ec45eda6df037e5f58b5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4b3fc2f122447484b98dbb10583889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "saes = []\n",
    "path_to_saes = \"results/sae/trained_models/\"\n",
    "\n",
    "for trainer in os.listdir(path_to_saes):\n",
    "    sae_cls = BatchTopKSAE if trainer == \"BatchTopKTrainer\" else AutoEncoderTopK\n",
    "    for model in tqdm(os.listdir(os.path.join(path_to_saes, trainer))):\n",
    "        path_to_sae = os.path.join(path_to_saes, trainer, model, \"trainer_0\")\n",
    "        sae = sae_cls.from_pretrained(os.path.join(path_to_sae, \"ae.pt\"))\n",
    "        d = sae.decoder.weight.shape[1]\n",
    "\n",
    "        path_to_interpretation = os.path.join(\"results/sae/interpretation/\", trainer, model, \"trainer_0\")\n",
    "        identity_counts = torch.load(os.path.join(path_to_interpretation, f\"identity_counts_d{d}_n100.pt\"))\n",
    "        identity_counts = identity_counts[:, selected_identity_indices]\n",
    "\n",
    "        top_n_text_embeddings = torch.load(os.path.join(path_to_interpretation, \"top_activating_texts_embeddings.pt\"))\n",
    "        top_n_text_embeddings = top_n_text_embeddings.cuda().float()\n",
    "        top_n_text_embeddings = top_n_text_embeddings[:, :50, :].mean(dim=1)\n",
    "        saes.append((sae, identity_counts, top_n_text_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e310072a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d836e57d09d24cb48f1da461eeec0163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_topic_clusters(topic_labels, cdist_cos, threshold):\n",
    "    topic_clusters = cluster_by_similarity(topic_labels, cdist_cos, threshold)\n",
    "    topic_clusters = [\", \".join(cluster) for cluster in topic_clusters]\n",
    "    topic_cluster_embeddings = embedder.encode(\n",
    "        topic_clusters, batch_size=4096, show_progress_bar=False, convert_to_tensor=True, normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    return topic_clusters, topic_cluster_embeddings\n",
    "\n",
    "def calculate_pmi(p_feature: torch.Tensor, p_identity_given_feature: torch.Tensor, p_topic_given_feature: torch.Tensor, epsilon: float = 1e-12) -> torch.Tensor:\n",
    "    # --- Step 1: Calculate marginal probabilities P(identity) and P(topic) ---\n",
    "    p_identity = (p_identity_given_feature * p_feature).sum(dim=0)\n",
    "    # Shape: (num_topics,)\n",
    "    p_topic = (p_topic_given_feature * p_feature).sum(dim=0)\n",
    "    # --- Step 2: Calculate the joint probability P(identity, topic) ---\n",
    "    p_joint_identity_topic = p_identity_given_feature.T @ (p_topic_given_feature * p_feature)\n",
    "    # --- Step 3: Calculate the denominator for the PMI formula: P(identity) * P(topic) ---\n",
    "    p_independent = torch.outer(p_identity, p_topic)\n",
    "    # --- Step 4: Calculate the final PMI matrix ---\n",
    "    ratio = p_joint_identity_topic / (p_independent + epsilon)\n",
    "    pmi_matrix = torch.log(ratio + epsilon)\n",
    "    return pmi_matrix\n",
    "\n",
    "def get_pmi(tau):\n",
    "    topic_clusters, topic_cluster_embeddings = get_topic_clusters(topic_labels, cdist_cos, tau)\n",
    "    global_pmi_matrix = torch.zeros(14, len(topic_clusters)).cuda().float()\n",
    "    topic_label_embeddings_normalized = torch.div(topic_cluster_embeddings, topic_cluster_embeddings.norm(dim=1, keepdim=True))\n",
    "    topic_label_embeddings_normalized = topic_label_embeddings_normalized.cuda().float()\n",
    "    \n",
    "    for sae, identity_counts, text_embeddings in saes:\n",
    "        # Calculate p(feature)\n",
    "        p_feature = identity_counts.sum(dim=-1)\n",
    "        p_feature = torch.div(p_feature, p_feature.sum(dim=0))\n",
    "        p_feature = p_feature.reshape(-1, 1).cuda().float()\n",
    "\n",
    "        # Calculate p(identity | feature)\n",
    "        identity_counts_smoothed = identity_counts + 1\n",
    "        identity_counts_smoothed_sum = identity_counts_smoothed.sum(dim=1, keepdim=True)\n",
    "        p_identity_given_feature = torch.div(identity_counts_smoothed, identity_counts_smoothed_sum)\n",
    "        p_identity_given_feature = p_identity_given_feature.cuda().float()\n",
    "    \n",
    "        # Calculate p(topic | feature) from decoder embeddings\n",
    "        decoder_embeddings = sae.decoder.weight.T.cuda().float()\n",
    "        #decoder_embeddings = text_embeddings\n",
    "        decoder_embeddings_normalized = torch.div(decoder_embeddings, decoder_embeddings.norm(dim=1, keepdim=True))\n",
    "        decoder_scores = torch.matmul(decoder_embeddings_normalized, topic_label_embeddings_normalized.T)\n",
    "        top_k_values, top_k_indices = torch.topk(decoder_scores, k=5, dim=1)\n",
    "        decoder_scores_pruned = torch.zeros_like(decoder_scores)\n",
    "        decoder_scores_pruned.scatter_(1, top_k_indices, top_k_values)\n",
    "        decoder_scores_pruned_smoothed = decoder_scores_pruned + 1e-8\n",
    "        p_topic_given_feature = torch.div(decoder_scores_pruned_smoothed, decoder_scores_pruned_smoothed.sum(dim=1, keepdim=True))\n",
    "\n",
    "        # Calculate PMI\n",
    "        pmi_matrix = calculate_pmi(p_feature, p_identity_given_feature, p_topic_given_feature)\n",
    "        # Replace nan with -inf\n",
    "        pmi_matrix = torch.where(torch.isnan(pmi_matrix), torch.full_like(pmi_matrix, -torch.inf), pmi_matrix)\n",
    "        global_pmi_matrix += pmi_matrix\n",
    "\n",
    "    global_pmi_matrix = torch.div(global_pmi_matrix, float(len(saes)))\n",
    "    return global_pmi_matrix, topic_clusters\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "identity_topic_scores = defaultdict(lambda: defaultdict(float))\n",
    "taus = np.linspace(0.8, 0.95, 20)\n",
    "\n",
    "for tau in tqdm(taus):\n",
    "    global_pmi_matrix, topic_clusters = get_pmi(tau)\n",
    "    topk_k_identity = torch.topk(global_pmi_matrix, 25, dim=1).indices\n",
    "    \n",
    "    for i, _ in enumerate(selected_identity_indices):\n",
    "        for idx in topk_k_identity[i].tolist():\n",
    "            topic = topic_clusters[idx]\n",
    "            tls = topic.split(\",\")\n",
    "            tls = [t.strip() for t in tls]\n",
    "\n",
    "            for t in tls:\n",
    "                identity_topic_scores[selected_identities[i]][t] += global_pmi_matrix[i, idx].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1290f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black_male &\n",
      "basketball (1.46), 3x3 basketball (1.29), canadian football (1.06), assault (1.04), basketball equipment (1.04), rugby league (1.03), american football (1.02), playoff championship (1.02), population growth (0.99), heptathlon (0.99), gaelic football (0.95), american football equipment (0.95), security measures (defense) (0.94), final game (0.91), sports officiating (0.90), decathlon (0.90), mormonism (0.89), fighting games (0.86), injury (0.84), rugby (0.82), \n",
      "eastasian_male &\n",
      "anime & manga (1.56), comics & animation (1.21), comics (1.20), action & platform games (1.16), martial arts (1.08), adventure games (1.08), mixed martial arts (1.05), sombo (martial art) (1.00), action & adventure films (0.95), animated films (0.95), video game development (0.94), confucianism (0.93), energy resources (0.89), acupuncture & chinese medicine (0.88), energy and resource (0.86), video game (0.84), nuclear energy (0.82), action figures (0.79), firearms & weapons (0.78), traditional chinese medicine (0.75), \n",
      "latino_male &\n",
      "bullfighting (1.26), lamborghini (1.25), enduro (1.24), soccer (1.19), mormonism (0.92), jaguar (0.89), soccer equipment (0.81), rugby (0.78), citroÃ«n (0.76), horseback riding (0.76), bull riding (0.75), continental cup (0.74), horse driving (0.72), canadian football (0.70), livestock (0.69), sports facilities (0.68), saddle bronc (0.62), rugby league (0.62), rugby union (0.62), sport venue (0.59), \n",
      "middleeastern_male &\n",
      "islam (1.57), firearms & weapons (1.40), sunni islam (1.34), shia islam (1.34), eid al-adha (1.30), military service (1.30), qur'an (1.27), military weaponry and equipment (1.26), military (1.23), civil and public service (1.22), zoroastrianism (1.19), combat sports equipment (1.15), military occupation (1.13), military equipment (1.12), drought (1.11), rifle shooting (1.10), judaism (1.04), civilian service (1.03), desserts (0.98), torah (0.90), \n",
      "southasian_male &\n",
      "kabaddi (2.10), cricket (1.90), hinduism (1.71), jainism (1.54), cricket equipment (1.53), bollywood & south asian films (1.52), sikhism (1.47), baseball (1.07), hornuss (1.00), judaism (0.94), shia islam (0.93), islam (0.93), international economic institution (0.87), public relations (0.85), international organization (0.84), political polls & surveys (0.82), bar and bat mitzvah (0.82), badminton (0.80), international relations (0.73), judge (0.70), \n",
      "southeastasian_male &\n",
      "java (programming language) (1.17), buddhism (1.08), sepak takraw (1.06), mixed martial arts (0.86), meat & seafood (0.85), meat & seafood substitutes (0.84), taoism (0.78), labor market (0.76), flexible work arrangements (0.73), work & labor issues (0.70), culinary training (0.66), waste management (0.66), ramadan (0.65), yard maintenance (0.61), market and exchange (0.59), construction & power tools (0.59), kickboxing (0.55), workplace health and safety (0.54), martial arts (0.53), occupational health & safety (0.51), \n",
      "white_male &\n",
      "male impotence (0.62), men's health (0.61), ice hockey (0.60), sledge hockey (0.59), aging & geriatrics (0.58), geriatric medicine (0.57), ageism (0.56), motorboat racing (0.52), men (0.50), senior citizens (0.47), auto racing (0.46), economy (0.45), seniors & retirement (0.42), motor car racing (0.40), motor sports (0.40), hockey (0.40), management (0.39), motorcycle racing (0.39), financial advisory service (0.38), business and finance (0.37), \n"
     ]
    }
   ],
   "source": [
    "gender_suffix = \"_male\"\n",
    "\n",
    "for i, identity in enumerate(selected_identities):\n",
    "    if not identity.endswith(gender_suffix):\n",
    "        continue\n",
    "    \n",
    "    topic_scores = identity_topic_scores[identity]\n",
    "    topic_scores = list(sorted(topic_scores.items(), key=lambda x: x[1], reverse=True))[:20]\n",
    "\n",
    "    print(identity, \"&\", end=\"\")\n",
    "    print()\n",
    "    for topic, score in topic_scores:\n",
    "        print(f\"{topic} ({score / 20:.2f}), \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a2d78c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "southeastasian_female\n",
      "\n",
      "1. java (programming language), programming\t1.03\n",
      "2. education, education policy\t0.90\n",
      "3. fruits & vegetables\t0.90\n",
      "4. acupuncture & chinese medicine, alternative & natural medicine, traditional chinese medicine\t0.88\n",
      "5. currencies & foreign exchange, foreign exchange market\t0.87\n",
      "6. sepak takraw\t0.86\n",
      "7. gardening, horticulture\t0.86\n",
      "8. cultural development, cultural policy, culture\t0.85\n",
      "9. plant disease\t0.84\n",
      "10. teaching & classroom resources, teaching and learning\t0.82\n",
      "11. kids & teens, teenagers\t0.76\n",
      "12. flowers, flowers and plants\t0.76\n",
      "13. medical tourism\t0.76\n",
      "14. farmers' markets, market and exchange\t0.75\n",
      "15. jobs, jobs & education\t0.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "identity_idx = 10\n",
    "\n",
    "print(selected_identities[identity_idx])\n",
    "print()\n",
    "\n",
    "topk_k_identity = torch.topk(global_pmi_matrix, 15, dim=1).indices[identity_idx].tolist()\n",
    "for i, idx in enumerate(topk_k_identity):\n",
    "    #print(topic_labels[idx] + \",\", end=\" \")\n",
    "    print(f\"{i+1}. {topic_clusters[idx]}\\t{global_pmi_matrix[identity_idx, idx]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
